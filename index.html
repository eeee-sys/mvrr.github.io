<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="keywords" content="Video-Language Understanding, Large Language Models">
  <meta name="description" content="MVRR is a Multi-Agent Framework for VideoQA">

  <title>Revise and Rewrite: A Self-Correcting Multi-Agent Framework for VideoQA</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-5C9M7TXSHJ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-5C9M7TXSHJ');
  </script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fontsource/nunito@5.0.18/index.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@1.0.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.2/css/fontawesome.min.css">
  <link rel="stylesheet" href="vendor/image-zoom.css">
  <link rel="stylesheet" href="index.css">
  <link rel="icon" href="assets/icon.png">

  <script async src="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.2/js/all.min.js"></script>
  <script async src="vendor/image-zoom.js"></script>
</head>

<body>
  <section class="hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <!--
          <img class="logo" src="assets/logo.png" alt="logo">
          -->
          <h1 class="title publication-title is-bold">
            <span>Revise and Rewrite: A Self-Correcting Multi-Agent Framework for VideoQA</span>
          </h1>
          <!--
          <div class="is-size-5 publication-author">
            <span class="author-block">
              <a href="https://yeliu.dev/" target="_blank">Ye Liu</a><sup>1&dagger;</sup>,
            </span>
            <span class="author-block">
              <a href="https://qhlin.me/" target="_blank">Kevin Qinghong Lin</a><sup>2&dagger;</sup>,
            </span>
            <span class="author-block">
              <a href="https://web.comp.polyu.edu.hk/chencw/" target="_blank">Chang Wen Chen</a><sup>1&#9993;</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/showlab" target="_blank">Mike Zheng Shou</a><sup>2&#9993;</sup>
            </span>
          </div>
          -->

          <!--
          <div class="is-size-5 publication-institution">
            <span class="author-block"><sup>1</sup>The Hong Kong Polytechnic University</span>
            <span class="author-block"><sup>2</sup>Show Lab, National University of Singapore</span>
          </div>
          -->

          <!--
          <div class="is-size-5 publication-role">
            <span class="author-block"><sup>&dagger;</sup><i>Equal Contribution</i></span>
            <span class="author-block"><sup>&#9993;</sup><i>Corresponding Authors</i></span>
          </div>
          -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!--
              <span class="link-block">
                <a class="button is-normal is-rounded is-dark" href="https://arxiv.org/abs/2503.13444" target="_blank">
                  <i class="button-icon far fa-paper-plane"></i>
                  <span>arXiv</span>
                </a>
              </span>
              -->
              <span class="link-block">
                <a class="button is-normal is-rounded is-dark" href="https://github.com/eeee-sys/MVRR" target="_blank">
                  <i class="button-icon fa-brands fa-github"></i>
                  <span>Code</span>
                </a>
              </span>
              <!--
              <span class="link-block">
                <a class="button is-normal is-rounded is-dark" href="https://huggingface.co/spaces/yeliudev/VideoMind-2B" target="_blank">
                  <i class="button-icon fa-regular fa-lightbulb"></i>
                  <span>Demo</span>
                </a>
              </span>
              -->
              <!--
              <span class="link-block">
                <a class="button is-normal is-rounded is-dark" href="https://huggingface.co/datasets/yeliudev/VideoMind-Dataset" target="_blank">
                  <i class="button-icon fa-regular fa-hourglass-half"></i>
                  <span>Dataset</span>
                </a>
              </span>
              -->
              <!--
              <span class="link-block">
                <a class="button is-normal is-rounded is-dark" href="https://huggingface.co/collections/yeliudev/videomind-67dd41f42c57f0e7433afb36" target="_blank">
                  <i class="button-icon fa-regular fa-envelope-open"></i>
                  <span>Checkpoints</span>
                </a>
              </span>
              -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title">Abstract</h2>
          <div class="content has-text-justified">
            <p>Video Question Answering (VideoQA) is a multimodal task that requires generating natural language responses based on visual 
              content and user inquiries. With the rapid advancement of Multimodal Large Language Models (MLLMs), MLLM-based approaches have 
              emerged as the dominant paradigm in VideoQA.</p>
            <p>However, MLLMs inherit a critical intrinsic limitation from their LLM precursors: hallucination, the generation of content that is 
              nonsensical or unfaithful to the source input. Consequently, ensuring result reliability is paramount for all LLM-based methodologies, and 
              VideoQA is no exception.</p>
            <p>To address this, we propose a novel framework MVRR designed to explicitly detect and mitigate hallucinations within MLLM-based 
              VideoQA systems. MVRR comprises four core components, which are the base VideoQA/MLLM, the Reviser, the Grounder, and the Rewriter, 
              integrated with two feedback loops. All modules share a unified MLLM backbone to maintain consistency. Within this architecture, the 
              Reviser plays a central role in error detection and workflow orchestration. The Grounder and Rewriter are responsible for optimizing 
              the video representation and the user query, respectively. By enhancing the quality of the prompt sequence, they indirectly improve the 
              context provided to the model, thereby suppressing hallucinations to a certain extent. Given that the Grounder and Rewriter are themselves 
              based on MLLMs, they possess a potential susceptibility to hallucination. To mitigate error propagation, we adopt a principle of progressive 
              intervention, where auxiliary modules are invoked only when strictly necessary. Furthermore, the framework features an extensible open architecture. 
              Beyond the Grounder and Rewriter, it can readily accommodate additional modules to inject supplementary auxiliary information into the prompt sequence. 
              Our method has been proved to be efficient on several benchmarks.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-half">
          <h2 class="title">What is MVRR?</h2>
          <img src="figures/teaser.png" alt="Teaser" data-zoom-image />
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title">Model Overview</h2>
          <img src="figures/method.png" alt="Model Overview" data-zoom-image />
        </div>
      </div>
    </div>
  </section>

    <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title">Architecture</h2>
          <img src="figures/arc.png" alt="Architecture" data-zoom-image />
        </div>
      </div>
    </div>
  </section> -->

  <!--
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title">Role-Specific Designs</h3>
        </div>
      </div>
      <div class="columns is-centered has-text-centered">
        <div class="column is-two-fifths">
          <img src="figures/decoder.jpg" alt="Timestamp Decoder" data-zoom-image />
          <div class="caption">Timestamp Decoder</div>
        </div>
        <div class="column is-two-fifths">
          <img src="figures/planner.jpg" alt="Planner" data-zoom-image />
          <div class="caption">Planner</div>
          <img src="figures/verifier.jpg" alt="Verifier" data-zoom-image />
          <div class="caption">Verifier</div>
        </div>
      </div>
    </div>
  </section>
  -->

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title">Visualization</h2>
          <img src="figures/vis.png" alt="Visualization" data-zoom-image />
        </div>
      </div>
    </div>
  </section>

  <!--
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title">Citation</h3>
          <div class="caption">Please kindly cite our paper if you find this project helpful.</div>
        </div>
      </div>
      <div class="columns is-centered">
        <pre><code>@article{liu2025videomind,<br>  title={VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning},<br>  author={Liu, Ye and Lin, Kevin Qinghong and Chen, Chang Wen and Shou, Mike Zheng},<br>  journal={arXiv preprint arXiv:2503.13444},<br>  year={2025}<br>}</code></pre>
      </div>
    </div>
  </section>
  -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <p>This website is modified from <a href="https://nerfies.github.io/" target="_blank">Nerfies's Project Page</a>. Source code is licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a>.</p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
