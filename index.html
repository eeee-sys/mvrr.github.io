<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="keywords" content="Video-Language Understanding, Large Language Models">
  <meta name="description" content="MVRR is a Multi-Agent Framework for VideoQA">

  <title>Revise and Rewrite: A Self-Correcting Multi-Agent Framework for VideoQA</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-5C9M7TXSHJ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-5C9M7TXSHJ');
  </script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fontsource/nunito@5.0.18/index.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@1.0.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.2/css/fontawesome.min.css">
  <link rel="stylesheet" href="vendor/image-zoom.css">
  <link rel="stylesheet" href="index.css">
  <link rel="icon" href="assets/icon.png">

  <script async src="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.2/js/all.min.js"></script>
  <script async src="vendor/image-zoom.js"></script>
</head>

<body>
  <section class="hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <!--
          <img class="logo" src="assets/logo.png" alt="logo">
          -->
          <h1 class="title publication-title is-bold">
            <span>Revise and Rewrite: A Self-Correcting Multi-Agent Framework for VideoQA</span>
          </h1>
          <!--
          <div class="is-size-5 publication-author">
            <span class="author-block">
              <a href="https://yeliu.dev/" target="_blank">Ye Liu</a><sup>1&dagger;</sup>,
            </span>
            <span class="author-block">
              <a href="https://qhlin.me/" target="_blank">Kevin Qinghong Lin</a><sup>2&dagger;</sup>,
            </span>
            <span class="author-block">
              <a href="https://web.comp.polyu.edu.hk/chencw/" target="_blank">Chang Wen Chen</a><sup>1&#9993;</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/showlab" target="_blank">Mike Zheng Shou</a><sup>2&#9993;</sup>
            </span>
          </div>
          -->

          <!--
          <div class="is-size-5 publication-institution">
            <span class="author-block"><sup>1</sup>The Hong Kong Polytechnic University</span>
            <span class="author-block"><sup>2</sup>Show Lab, National University of Singapore</span>
          </div>
          -->

          <!--
          <div class="is-size-5 publication-role">
            <span class="author-block"><sup>&dagger;</sup><i>Equal Contribution</i></span>
            <span class="author-block"><sup>&#9993;</sup><i>Corresponding Authors</i></span>
          </div>
          -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!--
              <span class="link-block">
                <a class="button is-normal is-rounded is-dark" href="https://arxiv.org/abs/2503.13444" target="_blank">
                  <i class="button-icon far fa-paper-plane"></i>
                  <span>arXiv</span>
                </a>
              </span>
              -->
              <span class="link-block">
                <a class="button is-normal is-rounded is-dark" href="https://github.com/eeee-sys/MVRR" target="_blank">
                  <i class="button-icon fa-brands fa-github"></i>
                  <span>Code</span>
                </a>
              </span>
              <!--
              <span class="link-block">
                <a class="button is-normal is-rounded is-dark" href="https://huggingface.co/spaces/yeliudev/VideoMind-2B" target="_blank">
                  <i class="button-icon fa-regular fa-lightbulb"></i>
                  <span>Demo</span>
                </a>
              </span>
              -->
              <!--
              <span class="link-block">
                <a class="button is-normal is-rounded is-dark" href="https://huggingface.co/datasets/yeliudev/VideoMind-Dataset" target="_blank">
                  <i class="button-icon fa-regular fa-hourglass-half"></i>
                  <span>Dataset</span>
                </a>
              </span>
              -->
              <!--
              <span class="link-block">
                <a class="button is-normal is-rounded is-dark" href="https://huggingface.co/collections/yeliudev/videomind-67dd41f42c57f0e7433afb36" target="_blank">
                  <i class="button-icon fa-regular fa-envelope-open"></i>
                  <span>Checkpoints</span>
                </a>
              </span>
              -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title">Abstract</h2>
          <div class="content has-text-justified">
            <p>Video question answering (VideoQA) requires multimodal large language models (MLLMs) to correctly 
              respond to user queries while jointly considering multimodal inputs and textual instructions.</p>
            <p>However, the performance of VideoQA in practical applications often fails to meet expectations, 
              with accuracy levels not being as high as anticipated. Several factors contribute to this issue: 1) 
              The inherent complexity of multi-modal inputs, combined with the unique temporal dimension of video d
              ata, which introduces challenges in effectively processing and aligning both visual 
              and textual information. 2) The increased computational burden imposed by long video inputs, 
              which results in the model struggling to focus on the most relevant segments, leading to a 
              dilution of attention and a decreased ability to generate accurate responses. 3) The lack of 
              clarity and redundancy in user text queries, which adds unnecessary complexity to the model’s 
              understanding process. This excessive cognitive load prevents the model from fully grasping the 
              user’s intended meaning, leading to a higher likelihood of errors in answering.</p>
            <p>In this work, we propose MVRR, an approach that unifies video grounding, 
              question answering, answer revising, and question rewriting. The framework would give initial 
              answer through grounding the most relevant video segments and answering the user query  
              based on the segments and the user's original query. Then, the framework would check whether 
              the initial answer is right or wrong. If the initial answer is wrong, MVRR further refines the 
              user query by removing redundant information, thereby simplifying the input. It then use the 
              rewritten question to perform secondary answering. MVRR significantly improves VideoQA 
              accuracy and demonstrates strong performance across multiple benchmarks, highlighting the 
              effectiveness of our approach.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-half">
          <h2 class="title">What is MVRR?</h2>
          <img src="figures/teaser.png" alt="Teaser" data-zoom-image />
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title">Model Overview</h2>
          <img src="figures/method.png" alt="Model Overview" data-zoom-image />
        </div>
      </div>
    </div>
  </section>

    <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title">Architecture</h2>
          <img src="figures/arc.png" alt="Architecture" data-zoom-image />
        </div>
      </div>
    </div>
  </section>

  <!--
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title">Role-Specific Designs</h3>
        </div>
      </div>
      <div class="columns is-centered has-text-centered">
        <div class="column is-two-fifths">
          <img src="figures/decoder.jpg" alt="Timestamp Decoder" data-zoom-image />
          <div class="caption">Timestamp Decoder</div>
        </div>
        <div class="column is-two-fifths">
          <img src="figures/planner.jpg" alt="Planner" data-zoom-image />
          <div class="caption">Planner</div>
          <img src="figures/verifier.jpg" alt="Verifier" data-zoom-image />
          <div class="caption">Verifier</div>
        </div>
      </div>
    </div>
  </section>
  -->

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title">Visualization</h2>
          <img src="figures/vis.png" alt="Visualization" data-zoom-image />
        </div>
      </div>
    </div>
  </section>

  <!--
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title">Citation</h3>
          <div class="caption">Please kindly cite our paper if you find this project helpful.</div>
        </div>
      </div>
      <div class="columns is-centered">
        <pre><code>@article{liu2025videomind,<br>  title={VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning},<br>  author={Liu, Ye and Lin, Kevin Qinghong and Chen, Chang Wen and Shou, Mike Zheng},<br>  journal={arXiv preprint arXiv:2503.13444},<br>  year={2025}<br>}</code></pre>
      </div>
    </div>
  </section>
  -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <p>This website is modified from <a href="https://nerfies.github.io/" target="_blank">Nerfies's Project Page</a>. Source code is licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a>.</p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
